{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH2YExQa16Gd"
      },
      "source": [
        "# CPSC 477/577 Spring 2025\n",
        "\n",
        "Instructor: Arman Cohan  \n",
        "Homework 2: Transformers and transfer learning\n",
        "\n",
        "### Part 1: Implementation of a transformer model for language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0Ng1L3puxz"
      },
      "source": [
        "### Please write your name and NetID below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtwa1W61px8Z"
      },
      "source": [
        "NAME: Yuan Chang\n",
        "\n",
        "NetID:yc2238\n",
        "\n",
        "\n",
        "**Instructions**: Read the notebook carefully, then add your implementation to places identified with `TODO` and also answer the Reflection questions.\n",
        "\n",
        "\n",
        "**IMPORTANT SUBMISSION NOTE 1**: Please submit your notebook having run all cells and hide long debugging output before submission.\n",
        "\n",
        "\n",
        "**IMPORTANT SUBMISSION NOTE 2**: If you included print or debugging statements, please remove those before your final run and submission.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DnTGzVwVl1k"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this assingment we will implement the transformer model architecture for language modeling from scratch.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMYqZpD-5Nq5"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "For this assignment, we will use Google Colab.\n",
        "\n",
        "#### Using GPU in Colab\n",
        "PyTorch and other deep learning libraries are much faster using GPU acceleration. For training and evaluating the models in this assignment, you should always use a GPU:\n",
        "\n",
        "1. Go to __Runtime__ option on the top left\n",
        "2. Click __Change runtime type__\n",
        "3. Select \"GPU\" for __Hardware\n",
        " accelerator__\n",
        "4. Click __SAVE__ button\n",
        "\n",
        "However, Colab limits the amount of time that you can use a free GPU.\n",
        "So you may wish to implement much of the assignment without the GPU. But note that you will have to run all cells again once you change the runtime type.\n",
        "You can also connect Colab to your local GPU for faster iteration.\n",
        "\n",
        "Alternatively the course is already setup on HPC, so you can access the jupyter notebook functionality there and run your code there.\n",
        "\n",
        "Colab has popular libraries already installed such as Pytorch, TensorFlow, OpenCV and Keras. Let's get started and verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "085UtXq2Vlp-",
        "outputId": "cbf01a4f-2363-45e1-8337-eebaf62cba71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers tokenizers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from typing import Tuple, List\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# We'll set the random seeds for deterministic results.\n",
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class Placeholder:\n",
        "    @property\n",
        "    def DO(self):\n",
        "        raise NotImplementedError(\"You haven't yet implemented this part of the assignment yet\")\n",
        "\n",
        "TO = Placeholder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4vDTpUW9ViV_",
        "outputId": "2a7b4b1e-5517-46cc-d2b7-becaeffbf46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version is:  2.5.1+cu124\n",
            "You are using:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Pytorch version is: \", torch.__version__)\n",
        "print(\"You are using: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aARw1Q0eiih"
      },
      "source": [
        "## 1. Transformers\n",
        "\n",
        "First, we will take a look into the famous *Transformer* model, which is foundation of LLMs such as current GPT-4o, Claude and more conventional models such as GPT-3, BERT, etc.\n",
        "We will use it in the following parts of this assignment.\n",
        "\n",
        "Transformers were introduced in the paper [\"Attention is all you need\" (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762). As the paper title suggests, the key idea that makes transformers work is *attention*. If you want to review attention and transformers, some useful resources include\n",
        "\n",
        "- the [original paper](https://arxiv.org/abs/1706.03762)\n",
        "- chapters 9 and 10 of Jurafsky & Martin\n",
        "- the blog posts [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) and [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/). (The first of these doesn't cover transformers but is useful for understanding attention.)\n",
        "http://nlp.seas.harvard.edu/annotated-transformer/\n",
        "- Youtube videos such as [this one](https://youtu.be/OyFJWRnt_AY) or [this one](https://youtu.be/iDulhoQ2pro)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3_Pi2STrlAs"
      },
      "source": [
        "### 1.1 Model Input\n",
        "\n",
        "Recall that in transfomer models, the input is a sequence of tokens.\n",
        "Concretely here is the input pipeline for a transformer model (or most of the neural network models in NLP):\n",
        "\n",
        "1- Given an input text $x$, we first tokenize it into a sequence of tokens. Tokens can be words or sub-word units (or even characters).\n",
        "We can assume that we have access to a blackbox tokenization method that given an input text $x$, returns a sequence of tokens $x_1, x_2, \\ldots, x_n$.\n",
        "\n",
        "2- The tokens are then converted into a sequence of token IDs. Each token ID is an integer that represents the token in the vocabulary.\n",
        "\n",
        "3- The token IDs are then converted into a sequence of token embeddings. Each token is represented as a vector, and the sequence of vectors is called an *embedding*.\n",
        "\n",
        "4- We can also optionally include additional information such as the position of each token in the sequence. This is done by adding a position embedding to the token embedding.\n",
        "Here each position is an integer that represents the position of the token in the sequence and then a separate position embedding matrix is used to look up the position embedding for each token.\n",
        "\n",
        "Each token is represented as a vector, and the sequence of vectors is called an *embedding*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNkzSDW2JkFG"
      },
      "source": [
        "### 1.1.1 Tokenization\n",
        "\n",
        "We assume we have access to a blackbox tokenization method that given an input text $x$, returns a sequence of tokens $x_1, x_2, \\ldots, x_n$.\n",
        "For this we use the `tokenizers` library from HuggingFace. This library provides a wide range of tokenizers for different languages and models.\n",
        "We will use the `GPT2Tokenizer` for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6mZTkgpVJkFG",
        "outputId": "8cbbbaf8-b5a0-44a4-fbce-6332a7ec334c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636,
          "referenced_widgets": [
            "79c87f52018b4af48f1d8834aae4a930",
            "718f47b73c1d4d49a68b3b71add22c0e",
            "3cccf3e7d6894118bc59e2c6e5890bad",
            "5812354c13c143769cb64ef5dfd6ca5e",
            "bb6977ef2a3246bbb288d2ec0f883108",
            "3b15943292a74cd4b98e882fcea2a687",
            "ef5982e589ec495ca257ed3c319b3f10",
            "2cd02f9debaa4d788828fcfcd88fdb81",
            "d067a48333bb405ea4e04e9ddba0e4d7",
            "8ae18816896f4e378540467c0c238b20",
            "61853934e9564267b2fefb9c334c4960",
            "de82744b68cb488b942b7d169dc70c9d",
            "5f15e9aca036444b85f52c5a1c03b625",
            "6f3e7c85dc544ace866ba318af08fdeb",
            "86fae1252b174c34946a31497c16dddb",
            "823319e59a5f44128c4aea1a8a818ffb",
            "da87436cbad442438ba56fc7cd3cca1c",
            "71c0234cb9af4975bc35c3763bfce18a",
            "be60ff4cb63d499da7509740fb87f383",
            "43906c367edc4a4db96bcca1f468cf8f",
            "efd626ac057246719caa52ab28be9f39",
            "c816fcd4f86441fb829aa3f62cba8a16",
            "d10c9f60745845bb8e9ae7effc107a72",
            "502ec6ac60fc468fb286d3df2ce2bae1",
            "b2424363ff0140b88d2a5cbd8889a5c4",
            "d78ad36a305f4645a6100fe8e656bf7a",
            "44226c88ba594504b2fe7e70917b6062",
            "7ac9566d091840a79e94baf9671b0ea2",
            "8b328047aad64bfaa74058800447ee33",
            "130db72de3d14a2b9e0ebf812e117cf1",
            "3e9ecfdacefe4f56b015c1e8ea4e81f4",
            "74a83a7765a84fb69442e297384990f0",
            "97f0f5d9bf2c4d479bc6fe9622a14467",
            "362b9344725e4882bb2f45784ae104db",
            "50edf0fdd855416f892bab6d4f26cf82",
            "c8cd605e2e904ae195754db8e7759afb",
            "dfcc9ad5dbce427ea86e0335b2243c68",
            "6c0a8fb4f7fc4a1e9f7a6b7f79af570d",
            "9fff74adf1f5403f8273ecc038207d14",
            "e1f60b9b16a34576a005e9953d1ecb5b",
            "50088f265ae442ef8ce05cc9428ebdc5",
            "4b56dcce689149e6aa64638991ed20a5",
            "d736cf2c91854dc9ad2c1a8aedac3059",
            "4249bb2c1a004fb49f79dc657c28bd3f",
            "30b3b11aa6264d518f5705ca703647c9",
            "537d4b0d6aee418db4231a985d3311ba",
            "7141128b489c4b80bef17e1c74f44946",
            "3842d3fee6ff4551ba5ef920419ccec6",
            "54e91d1f2f4b4352952f629d9572145b",
            "925768ba7d414416a87f3ac47700e50a",
            "828be382d11146668c88979b6feb5623",
            "dee0cb346114487884b457efd0d1f494",
            "1600ec833262406c851058c7142e8e69",
            "86dbbc556c8c442bb71f8d4620763d82",
            "11f6cb720cd54ca6b34c2bfc40baefe6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79c87f52018b4af48f1d8834aae4a930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de82744b68cb488b942b7d169dc70c9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d10c9f60745845bb8e9ae7effc107a72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "362b9344725e4882bb2f45784ae104db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b3b11aa6264d518f5705ca703647c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1212,\n",
              " 16711,\n",
              " 286,\n",
              " 262,\n",
              " 48584,\n",
              " 5337,\n",
              " 416,\n",
              " 406,\n",
              " 46470,\n",
              " 32709,\n",
              " 25494,\n",
              " 318,\n",
              " 17700,\n",
              " 306,\n",
              " 2342,\n",
              " 540,\n",
              " 475,\n",
              " 19556,\n",
              " 13]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"This adaptation of the enigmatic novel by Liane Moriarty is supremely watchable but flawed.\"\n",
        "\n",
        "# we can use tokenizer.encode to convert text to token IDs\n",
        "tokens_ids = tokenizer.encode(text)\n",
        "\n",
        "tokens_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hdtWqwfPJkFG",
        "outputId": "550d86af-ccfc-4fd1-d2a0-f2505888def1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'Ġadaptation',\n",
              " 'Ġof',\n",
              " 'Ġthe',\n",
              " 'Ġenigmatic',\n",
              " 'Ġnovel',\n",
              " 'Ġby',\n",
              " 'ĠL',\n",
              " 'iane',\n",
              " 'ĠMori',\n",
              " 'arty',\n",
              " 'Ġis',\n",
              " 'Ġsupreme',\n",
              " 'ly',\n",
              " 'Ġwatch',\n",
              " 'able',\n",
              " 'Ġbut',\n",
              " 'Ġflawed',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# now convert token IDs back to text to inspect the tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
        "\n",
        "tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7IFcU8sJkFG"
      },
      "source": [
        "You will notice some tokens start with a \"Ġ\".\n",
        "Popular tokenizers use a special symbol such as \"Ġ\" (BPE tokenizer such as GPT2) or \"▁\" (SentencePiece) to represent space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o21vD8R4JkFG"
      },
      "source": [
        "#### Converting token IDs to text\n",
        "\n",
        "We can also use the tokenizers library to convert token IDs back to text. This is useful for represting the output of the model in human readable form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4VkiRuatJkFG",
        "outputId": "8b65a15f-a247-4e5e-cf54-9adebde5d520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This adaptation of the enigmatic novel by Liane Moriarty is supremely watchable but flawed.\n"
          ]
        }
      ],
      "source": [
        "# we can use tokenizer.decode() method to convert token IDs back to text\n",
        "\n",
        "converted_text = tokenizer.decode(tokens_ids)\n",
        "print(converted_text)\n",
        "\n",
        "assert text == converted_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PsplodWJkFG"
      },
      "source": [
        "### 1.2 Embeddings\n",
        "\n",
        "We first need to implement the embedding layer of the transformer. Recall that this is layer 0. We will use the `nn.Embedding` layer in PyTorch to implement this.\n",
        "The `nn.Embedding` creates an embdding matrix of size `vocab_size x embedding_dim`. Given a sequence of token IDs, we can use the `nn.Embedding` layer to look up the token embeddings for given token IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBNhTQeVJkFG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wt_ILYvJJkFG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int):\n",
        "        \"\"\"\n",
        "        Initializes the embedding layer.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of unique tokens in the vocabulary.\n",
        "            d_model (int): The embedding dimension (size of each embedding vector).\n",
        "        \"\"\"\n",
        "        super(Embedding, self).__init__()\n",
        "\n",
        "        # 🔹 TODO: Define the embedding layer using nn.Embedding.\n",
        "        # 💡 Hint: The nn.Embedding layer should map 'vocab_size' tokens into 'd_model' embeddings.\n",
        "\n",
        "\n",
        "        self.wte = nn.Embedding(vocab_size,d_model)  # 🔻 REPLACE 'None' with your implementation\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass of the embedding layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): A tensor of token indices with shape (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The corresponding embeddings with shape (batch_size, sequence_length, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        # 🔹 TODO: Lookup embeddings for the given input indices.\n",
        "        # 💡 Hint: Use self.wte to retrieve embeddings.\n",
        "\n",
        "        return self.wte(x)  # 🔻 REPLACE 'None' with your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aRF35Fa2JkFG"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "vocab_size = 10\n",
        "d_model = 16\n",
        "\n",
        "embedding = Embedding(vocab_size, d_model)\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "output = embedding(x)\n",
        "assert output.shape == (4, d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ-mdp_hJkFG"
      },
      "source": [
        "# 1.3 Positional Embeddings\n",
        "\n",
        "Now we implement the positional embeddings. As mentioned above this is a learned position embedding method and we will use the `nn.Embedding` layer in PyTorch to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NQImKA7dtCHg"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int):\n",
        "        super(PositionalEmbeddings, self).__init__()\n",
        "\n",
        "        # 🔹 TODO: Implement\n",
        "        # create a tensor of shape (1, max_len, d_model) to store the positional embeddings using nn.Embedding\n",
        "        self.embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # add the positional embeddings to the input tensor\n",
        "        # 🔹 TODO: Implement\n",
        "        return self.embedding(x)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SRBDcz8LJkFH",
        "outputId": "3034020d-14e3-4b4a-937d-68eb41adcf5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5284,  2.0264,  0.2286, -0.3184,  0.0555, -0.8794, -0.2590,  1.2397,\n",
              "          1.0065,  0.9851, -0.4223,  0.1129, -0.5996, -0.0496,  3.0999, -0.3272],\n",
              "        [-0.8858, -0.1698, -0.9845, -1.0561,  1.5622,  1.2579, -1.7318,  0.9355,\n",
              "          1.7652, -0.1249,  1.4508, -0.5174, -0.2349, -0.3428,  0.0640,  1.2952],\n",
              "        [ 1.9605,  0.3797, -0.4056,  1.9377, -0.7430,  0.6795, -0.1050,  0.1765,\n",
              "          1.4710, -1.9942, -1.4419, -1.1211,  0.7427, -0.4917,  0.4131, -1.0259],\n",
              "        [-1.1577,  0.6924,  1.4399, -1.5901,  0.5713, -0.9030, -0.2885, -0.7355,\n",
              "          0.7207, -0.2414,  1.4590,  0.5052, -0.7366,  1.0573, -0.6979, -1.3404]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# tests\n",
        "\n",
        "d_model = 16\n",
        "max_len = 64\n",
        "\n",
        "positional_embeddings = PositionalEmbeddings(d_model, max_len)\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "output = positional_embeddings(x)\n",
        "assert output.shape == (4, d_model)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQd0bhV0JkFH"
      },
      "source": [
        "Now we put the token embeddings and positional embeddings together to get the input embeddings for the transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2b8-N5myJkFH"
      },
      "outputs": [],
      "source": [
        "# Combine the embeddings and positional embeddings\n",
        "\n",
        "class TokenEmbedder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_len: int):\n",
        "        super(TokenEmbedder, self).__init__()\n",
        "        self.token_embedding = Embedding(vocab_size, d_model)\n",
        "        self.positional_embedding = PositionalEmbeddings(d_model, max_len)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # add the token embeddings and positional embeddings together\n",
        "        pos = torch.arange(0, x.shape[1], dtype=torch.long) # shape: [sequence length]\n",
        "        return self.token_embedding(x) + self.positional_embedding(pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oaIrci0qJkFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468b10ea-08a9-4498-e0c5-100d718076c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 26, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# test the whole input pipeline\n",
        "\n",
        "sample_texts = [\"This adaptation of the enigmatic novel by Liane Moriarty is supremely watchable but flawed.\",\n",
        "                \"The story is a bit of a slow burn, but the performances are top-notch and the ending is worth the wait.\"]\n",
        "\n",
        "# encode the text\n",
        "vocab_size = tokenizer.vocab_size\n",
        "d_model = 64\n",
        "\n",
        "# return_tensors=\"pt\" returns pytorch tensors directly. truncation and padding are used to ensure the input length is the same\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "token_ids = tokenizer(sample_texts, return_tensors=\"pt\", max_length=64, padding=\"longest\", truncation=True)['input_ids']\n",
        "\n",
        "token_embedder = TokenEmbedder(vocab_size=tokenizer.vocab_size, d_model=768, max_len=64)\n",
        "\n",
        "# pass the token_ids to the token_embedder\n",
        "output = token_embedder(token_ids)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S468Z3HBJkFH"
      },
      "source": [
        "## 1.4 Reflection Questions\n",
        "\n",
        "1- What is the purpose of the positional embeddings? Why do we need them?\n",
        "\n",
        "🔹 TODO: Position embedding is to let the model learn the context information of the word. We need them because transformer doesnt have the ability to remember the position of the token, so we need to mannually add them to the embeddings.\n",
        "\n",
        "\n",
        "2- What is the purpose of this line?   \n",
        "`pos = torch.arange(0, x.shape[1], dtype=torch.long) # shape: [sequence length]`\n",
        "\n",
        "🔹 TODO: This line creates a tensor containing position indices ranging from 0 to the length of the tokens. This is used to give the position information.\n",
        "\n",
        "3- What is the shape of the output from the TokenEmbedder correspond to?\n",
        "\n",
        "🔹 TODO: it is batch size, sequence length, and the dimension of the model.\n",
        "\n",
        "\n",
        "4- Why do we add the positional embeddings to the token embeddings? Can we use other methods like concatenation?\n",
        "\n",
        "🔹 TODO: Addition allows the model to learn interactions between token meanings and positions directly, concatenation will increase the dimension, adding more cost of computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwY5sUNJkFH"
      },
      "source": [
        "## 2. Transformer Model Architecture\n",
        "\n",
        "Now that we have implemented the input pipeline we can move on to the transformer model architecture.\n",
        "\n",
        "We will take a modular approach to implementing the transformer model. We will implement the following components of the transformer model:\n",
        "\n",
        "1- QKV Projection\n",
        "\n",
        "2- Multi-head self-attention\n",
        "\n",
        "3- Position-wise feedforward network\n",
        "\n",
        "4- Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8m73CqGMFKZ"
      },
      "source": [
        "### 2.1 Projections\n",
        "\n",
        "We first want to create separate projections for the input.\n",
        "In class we implemented this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bCk5xiKGMFKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7924a098-60ea-4f31-9aad-b7d287c488e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 10, 64]) torch.Size([2, 8, 10, 64]) torch.Size([2, 8, 10, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QKVProjection(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_k):\n",
        "        super(QKVProjection, self).__init__()\n",
        "\n",
        "        assert num_heads * d_k == d_model, \"d_model must be equal to num_heads * d_k\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "        # Linear layers for Q, K, V projections\n",
        "        self.q_linear = nn.Linear(d_model, num_heads * d_k)\n",
        "        self.k_linear = nn.Linear(d_model, num_heads * d_k)\n",
        "        self.v_linear = nn.Linear(d_model, num_heads * d_k)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, seq_length, d_model = X.shape\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        # then reshape so that the result is of shape [batch_size, seq_len, num_heads, d_k]\n",
        "        Q = self.q_linear(X).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.k_linear(X).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.v_linear(X).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        return Q, K, V\n",
        "\n",
        "# Example usage:\n",
        "d_model = 512  # Embedding size\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_k = 64  # Dimension per head (num_heads * d_k must be d_model)\n",
        "\n",
        "batch_size = 2\n",
        "seq_length = 10\n",
        "\n",
        "# Random input tensor\n",
        "X = torch.rand(batch_size, seq_length, d_model)\n",
        "\n",
        "# Instantiate and apply QKVProjection\n",
        "qkv_projection = QKVProjection(d_model, num_heads, d_k)\n",
        "Q, K, V = qkv_projection(X)\n",
        "\n",
        "print(Q.shape, K.shape, V.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NanOyixdJkFH"
      },
      "source": [
        "## 2.2 Multi-head self-attention\n",
        "\n",
        "The self-attention mechanism is the key idea that makes transformers work. It allows the model to weigh the importance of different tokens in the input sequence when computing the output for each token.\n",
        "The main parameters of this layer are the projection matrices $W_Q, W_K, W_V$ and we have $H$ heads (for each head we have separate projections).\n",
        "Recall that the dimension of the input sequence is $d_{model}$ and the dimension of the output sequence is also $d_{model}$.\n",
        "And the dimension of the projected queries, keys and values is $d_k = d_v = d_{model} / H$.\n",
        "\n",
        "Based on this information we can implement the multi-head self-attention layer.\n",
        "\n",
        "Hint: For tensor multiplications you can use either `torch.matmul` or `torch.einsum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S-NV9SpGJkFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd40d5a8-dd9b-4b07-b609-ccc2a35edf24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.158s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 512])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f5f401d4210>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_k):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert num_heads * d_k == d_model, \"d_model must be equal to num_heads * d_k\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "        # Use the improved QKVProjection\n",
        "        self.qkv_projection = QKVProjection(d_model, num_heads, d_k)\n",
        "\n",
        "        # Final output projection\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform multi-head self-attention.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - attention_weights (torch.Tensor): Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
        "                - output (torch.Tensor): Output tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        # Generate Q, K, V using the projection module\n",
        "        Q, K, V = self.qkv_projection(X)\n",
        "\n",
        "        # 🔹 TODO: Ensure the following implementation is clear and complete:\n",
        "        batch_size, num_heads, seq_length, d_k = Q.shape\n",
        "\n",
        "        # 🔹 TODO: Compute scaled dot-product attention for the raw_scores:\n",
        "        #    a) Compute the raw attention scores using the dot product between Q and K.\n",
        "        #    b) Scale the scores by dividing by sqrt(d_k).\n",
        "        raw_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        # 🔹 TODO: Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(raw_scores, dim=-1)\n",
        "\n",
        "        # 🔹 TODO: Now we should compute the context vector\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # 🔹 TODO: Compute the output vecotor.\n",
        "        # Hint: Reshape the context tensor to match the original shape of X.\n",
        "        output = context.transpose(1, 2).contiguous().view(batch_size, seq_length, -1) # the context tensor is not contiguous after transpose, use.contiguous() to view\n",
        "\n",
        "        return attention_weights, output\n",
        "\n",
        "# Example usage:\n",
        "d_model = 512  # Embedding size\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_k = 64  # Dimension per head (num_heads * d_k must be d_model)\n",
        "\n",
        "batch_size = 2\n",
        "seq_length = 10\n",
        "\n",
        "# Random input tensor\n",
        "X = torch.rand(batch_size, seq_length, d_model)\n",
        "\n",
        "# Instantiate and apply Multi-Head Attention\n",
        "multi_head_attn = MultiHeadAttention(d_model, num_heads, d_k)\n",
        "_, C = multi_head_attn(X)\n",
        "\n",
        "print(C.shape)  # Expected: (batch_size, seq_length, d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------- Unit tests ---------------\n",
        "# -----------DO NOT EDIT THIS PART -----\n",
        "\n",
        "\n",
        "import unittest\n",
        "import torch\n",
        "\n",
        "class TestMultiHeadAttention(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.d_model = 512\n",
        "        self.num_heads = 8\n",
        "        self.d_k = 64\n",
        "        self.batch_size = 2\n",
        "        self.seq_length = 10\n",
        "        torch.manual_seed(42)\n",
        "        self.multi_head_attn = MultiHeadAttention(self.d_model, self.num_heads, self.d_k)\n",
        "        self.X = torch.rand(self.batch_size, self.seq_length, self.d_model, requires_grad=True)\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        _, output = self.multi_head_attn(self.X)\n",
        "        expected_shape = (self.batch_size, self.seq_length, self.d_model)\n",
        "        self.assertEqual(output.shape, expected_shape, f\"Expected shape {expected_shape}, but got {output.shape}\")\n",
        "\n",
        "    def test_deterministic_behavior(self):\n",
        "        torch.manual_seed(42)\n",
        "        _, output1 = self.multi_head_attn(self.X)\n",
        "        torch.manual_seed(42)\n",
        "        _, output2 = self.multi_head_attn(self.X)\n",
        "        self.assertTrue(torch.allclose(output1, output2, atol=1e-6), \"MultiHeadAttention is not deterministic!\")\n",
        "\n",
        "    def test_gradient_computation(self):\n",
        "        \"\"\"Ensure gradients are computed properly.\"\"\"\n",
        "        _, output = self.multi_head_attn(self.X)\n",
        "        loss = output.sum()  # Simple loss function\n",
        "        loss.backward()\n",
        "\n",
        "        self.assertIsNotNone(self.X.grad, \"Gradients were not computed for input!\")\n",
        "        self.assertGreater(self.X.grad.abs().sum().item(), 0, \"Gradient sum is zero!\")\n",
        "\n",
        "    def test_attention_softmax(self):\n",
        "        \"\"\"Ensure that the attention scores sum up to ~1.\"\"\"\n",
        "        attention_weights, _ = self.multi_head_attn(X)\n",
        "\n",
        "        # Sum of softmax probabilities along last dimension should be close to 1\n",
        "        attention_sum = attention_weights.sum(dim=-1)\n",
        "        ones = torch.ones_like(attention_sum)\n",
        "\n",
        "        self.assertTrue(torch.allclose(attention_sum, ones, atol=1e-6), \"Attention weights do not sum to 1!\")\n",
        "\n",
        "    def test_known_computation(self):\n",
        "        d_model = 4\n",
        "        num_heads = 1\n",
        "        d_k = 4\n",
        "        seq_length = 3\n",
        "        batch_size = 1\n",
        "        multi_head_attn = MultiHeadAttention(d_model, num_heads, d_k)\n",
        "        with torch.no_grad():\n",
        "            # For Q, K, V projections\n",
        "            for linear in [multi_head_attn.qkv_projection.q_linear,\n",
        "                           multi_head_attn.qkv_projection.k_linear,\n",
        "                           multi_head_attn.qkv_projection.v_linear,\n",
        "                           multi_head_attn.out_linear]:\n",
        "                linear.weight.copy_(torch.eye(d_model))\n",
        "                if linear.bias is not None:\n",
        "                    linear.bias.zero_()\n",
        "        X_known = torch.ones(batch_size, seq_length, d_model)\n",
        "        attention_weights, output = multi_head_attn(X_known)\n",
        "        expected_output = torch.ones(batch_size, seq_length, d_model)\n",
        "        self.assertTrue(torch.allclose(output, expected_output, atol=1e-6),\n",
        "                        f\"Expected output {expected_output}, but got {output}\")\n",
        "\n",
        "unittest.main(argv=[''], exit=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvimXYryJkFH"
      },
      "source": [
        "## 2.3 Feedforward network\n",
        "\n",
        "The feedforward network is a simple two-layer neural network with a ReLU activation function in between the layers.\n",
        "The main parameters of this layer are the weight matrices $W_1, W_2$ and the bias vectors $b_1, b_2$.\n",
        "This layer takes as input a sequence of vectors of dimension $d_{model}$ and returns a sequence of vectors of the same dimension.\n",
        "This layer is applied to each position in the sequence independently.\n",
        "\n",
        "We'll next implement this module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4W0Mf95sJkFH"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 🔹 TODO Implement the init function. Recall that we need 2 linear layers. We can use nn.Linear\n",
        "        self.a = nn.Linear(d_model, d_ff)\n",
        "        self.b = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # 🔹 TODO: Implement the forward pass\n",
        "        # YOUR CODE HERE\n",
        "        x = self.a(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.b(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4eX2AAdJkFH"
      },
      "source": [
        "## 2.4 Layer normalization\n",
        "\n",
        "Layer normalization is a simple normalization technique that is applied to the output of each sub-layer in the transformer model.  \n",
        "Recall that the main hyperparameters of this layer are the scaling and shifting parameters $\\gamma, \\beta$.  \n",
        "Next we implement this module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ObMpeCHAJkFH"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps: float):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # 🔹 TODO: Implement the layer normalization\n",
        "        # hint: use the formula from the lecture slides and recall the shape of the tensor x: [batch_size, seq_len, d_model]\n",
        "        # YOUR CODE HERE\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.1 Causal Multi Head Attention\n",
        "\n",
        "Recall that for language modeling we need to prevent multi-head attention to attend to future tokens. We do this with a causal mask.\n",
        "\n",
        "In this part you need to do the following.\n",
        "1- Subclass MultiHeadAttention.\n",
        "2- Extend the Forward Method to accept a `causal_mask: bool` and `attn_pdrop` arguments. `attn_pdrop` is the probability of dropout applied to the attention weights. This helps with regularization during training. You can apply dropout with probability attn_pdrop to the attention weights before computing the final context vector.\n",
        "\n",
        "Complete the forward function below.\n"
      ],
      "metadata": {
        "id": "GDcbju83QKyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalMultiHeadAttention(MultiHeadAttention):\n",
        "\n",
        "    def forward(self, X, causal_mask: bool = False, attn_pdrop: float = 0.1):\n",
        "        # 🔹 TODO: Implement the forward function\n",
        "        # Hint: You can use torch.tril or torch.triu https://pytorch.org/docs/stable/generated/torch.tril.html#torch-tril\n",
        "        # YOUR CODE HERE\n",
        "        Q, K, V = self.qkv_projection(X)\n",
        "        batch_size, num_heads, seq_length, d_k = Q.shape\n",
        "        raw_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if causal_mask:\n",
        "            mask = torch.tril(torch.ones(seq_length, seq_length, device=X.device)).view(1, 1, seq_length, seq_length)\n",
        "            raw_scores = raw_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(raw_scores, dim=-1)\n",
        "        attention_weights = F.dropout(attention_weights, p=attn_pdrop, training=self.training)\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        output = context.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
        "        output = self.out_linear(output)\n",
        "        return attention_weights, output\n",
        ""
      ],
      "metadata": {
        "id": "hLMqiUbKRBJS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_k = 64\n",
        "seq_length = 10\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "X = torch.rand(batch_size, seq_length, d_model)\n",
        "causal_attn = CausalMultiHeadAttention(d_model, num_heads, d_k)\n",
        "attn_weights, output = causal_attn(X, causal_mask=True)\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXSMiNLjJjS6",
        "outputId": "0166b323-c5fb-4701-c876-b1dd2cf09d84"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm6NeQ5NJkFH"
      },
      "source": [
        "## 2.5 Transformer Block\n",
        "\n",
        "Now we can put the multi-head self-attention and the feedforward network together to implement the transformer block.  \n",
        "Recall that we also need to implement the layer normalization and the residual connections.\n",
        "\n",
        "\n",
        "You are provided with a code skeleton for the TransformerBlock class. Your task is to implement the forward method to correctly combine the attention mechanism, feed-forward network, residual connections, and layer normalization.\n",
        "Pay close attention to the following:\n",
        "\n",
        "Input Parameters:\n",
        "\n",
        "`d_model`: The dimension of the input embeddings.   \n",
        "`num_heads`: The number of attention heads.  \n",
        "`attn_pdrop`: The dropout probability used in the attention module.  \n",
        "`dropout`: The dropout probability for the feed-forward network.  \n",
        "`d_ff`: The hidden dimension size in the feed-forward network.  \n",
        "`eps`: A small constant for numerical stability in layer normalization.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cvqhGjxMJkFH"
      },
      "outputs": [],
      "source": [
        "# This uses your previous implementation of the previous blocks.\n",
        "# Remember that we need to use the MultiHeadAttention, FeedForward, and LayerNorm modules that you implemented earlier\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, attn_pdrop: float, dropout: float, d_ff: int, eps: float):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # 🔹 TODO: initialize and use any of the modules that are required for a Transformer Block\n",
        "        # YOUR CODE HERE\n",
        "        self.attn = CausalMultiHeadAttention(d_model, num_heads, d_model // num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = LayerNorm(d_model, eps)\n",
        "        self.layer_norm2 = LayerNorm(d_model, eps)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor, causal_mask: bool) -> Tensor:\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the transformer block.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (batch_size, seq_length, d_model).\n",
        "            causal_mask (bool): If True, apply a causal mask in the self-attention layer\n",
        "                                (useful for decoder self-attention to prevent attending to future tokens).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output of the transformer block with shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        # 🔹 TODO Implement the forward pass\n",
        "        # hint: the forward pass is similar to the one in the lecture slides. You should use the MultiHeadAttention and FeedForward modules that you implemented earlier\n",
        "        # hint: remember to use residual connections (x + layer_output) after attention and feed-forward\n",
        "        # hint: remember to apply layer normalization before attention and feed-forward\n",
        "        # hint: remember to apply dropout after attention and feed-forward but before residual\n",
        "        # YOUR CODE HERE\n",
        "        attn_weights, attn_output = self.attn(self.layer_norm1(x), causal_mask=causal_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        x = x + attn_output\n",
        "        ff_output = self.ff(self.layer_norm2(x))\n",
        "        ff_output = self.dropout2(ff_output)\n",
        "        ff_output = attn_output + ff_output\n",
        "        return ff_output\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPtTEKSJkFH"
      },
      "source": [
        "## 2.6 Transformer Stack\n",
        "\n",
        "We can now put the transformer blocks on top of each other to implement the transformer stack.  \n",
        "In this part we will simply stack the transformer blocks on top of each other to implement the transformer stack.\n",
        "\n",
        "**Key Concepts**\n",
        "- Sequential processing: Each layer builds upon the representations learned by previous layers\n",
        "- Parameters of layers: All layers share the same architecture but have different learned parameters\n",
        "- Depth vs width: The number of layers (depth) is crucial for model capacity, while d_model represents width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Jm0Dt1ZeJkFH"
      },
      "outputs": [],
      "source": [
        "class TransformerStack(nn.Module):\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, attn_pdrop: float, dropout: float, d_ff: int, eps: float):\n",
        "        \"\"\"\n",
        "        A stack of Transformer blocks that processes input sequentially through multiple layers.\n",
        "        This architecture allows for deep hierarchical processing of input sequences.\n",
        "\n",
        "        Architecture:\n",
        "            Input → TransformerBlock₁ → TransformerBlock₂ → ... → TransformerBlockₙ → Output\n",
        "        \"\"\"\n",
        "        super(TransformerStack, self).__init__()\n",
        "\n",
        "        # 🔹 TODO Implement\n",
        "        # hint: you should create a list of TransformerBlock modules and store it in self.layers\n",
        "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, attn_pdrop, dropout, d_ff, eps) for _ in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, causal_mask: bool) -> Tensor:\n",
        "        \"\"\"\n",
        "        Process input through all transformer blocks sequentially.\n",
        "\n",
        "        Implementation steps:\n",
        "        1. Iterate through each layer in the stack\n",
        "        2. Pass the output of each layer as input to the next layer\n",
        "        3. Maintain the causal mask throughout if specified\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_length, d_model)\n",
        "            causal_mask (bool): If True, apply causal masking in all attention layers\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Processed output of shape (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        # 🔹 TODO: Implement\n",
        "        # YOUR CODE HERE\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal_mask)\n",
        "        return x   # 🔻 REPLACE x with whatever your implementation returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYC7SpIJkFI"
      },
      "source": [
        "## 2.7 Building a Complete Transformer Model\n",
        "\n",
        "Finally we can put the input embeddings and the transformer stack together to implement the transformer model.  \n",
        "This includes the input pipeline, the transformer stack and the output layer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XaecoZ7hJkFQ"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer model for language modeling tasks.\n",
        "\n",
        "    Architecture:\n",
        "        Input Tokens → Token Embeddings → Transformer Stack → Language Model Head → Output Logits\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, attn_pdrop: float, dropout: float, d_ff: int, max_len: int, num_layers: int, eps: float):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        # 🔹 TODO: Implement\n",
        "        # hint: you should create the TokenEmbedder and TransformerStack modules\n",
        "        # hint: at the end, you should add a linear layer to convert the transformer output to the vocabulary size\n",
        "        # this is the language model head which allows us to predict the next token in the sequence\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.token_embedder = TokenEmbedder(vocab_size, d_model, max_len)\n",
        "        self.transformer_stack = TransformerStack(num_layers, d_model, num_heads, attn_pdrop, dropout, d_ff, eps)\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, causal_mask: bool) -> Tensor:\n",
        "        # 🔹 TODO: Implement\n",
        "        # YOUR CODE HERE\n",
        "        x = self.token_embedder(x)\n",
        "        x = self.transformer_stack(x, causal_mask)\n",
        "        x = self.output_layer(x)\n",
        "        return x # 🔻 REPLACE None with whatever your implementation returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vSdpon8og_l"
      },
      "source": [
        "Let's test your model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8GDOGZmiolfp"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "attn_pdrop = 0.1\n",
        "dropout = 0.1\n",
        "d_ff = 3072\n",
        "max_len = 20\n",
        "num_layers = 12\n",
        "eps = 1e-6\n",
        "\n",
        "# create the model\n",
        "model = TransformerModel(vocab_size, d_model, num_heads, attn_pdrop, dropout, d_ff, max_len, num_layers, eps)\n",
        "\n",
        "token_ids = tokenizer(sample_texts, return_tensors=\"pt\", max_length=max_len, padding=\"longest\", truncation=True)['input_ids']\n",
        "output = model(token_ids, causal_mask=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jub3IPKyJkFQ"
      },
      "source": [
        "## 3. Reflection questions\n",
        "\n",
        "Please answer the following questions in the markdown cells below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43V0-EHAJkFQ"
      },
      "source": [
        "1- What is the purpose of the output projection $W_O$ in the transformer model?\n",
        "\n",
        "TODO: The purpose of the output projection $W_O$ is to combine all the attention heads attention result to a single result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV4uhwtRJkFQ"
      },
      "source": [
        "2 - What is the total number of trainable parameters in the multi-head self-attention layer that you implemneted above?  \n",
        "Provide the final answer first, and then show your work.\n",
        "\n",
        "TODO: 2362368\n",
        "it is 4$\\cdot$768$\\cdot$768 + 4$\\cdot$768 = 2362368"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAY0DgRiceCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBpIzY2RJkFQ"
      },
      "source": [
        "3- What is the time complexity of the multi-head self-attention layer that you implemented above?  \n",
        "Provide your answer in terms of the sequence length $n$, the dimension of the input sequence $d_{model}$.\n",
        "Also show your work on how you arrived at your answer.\n",
        "\n",
        "TODO: For Computing the QKV, the time complexity is O($nd_{model}^2$)  \n",
        "For Computing the score, it is still O($nd_{model}^2$/$h$) O($nd_{model}^2$)  \n",
        "For Computing the Softmax, it is O($n^2$)\n",
        "\n",
        "So, the Overall time complexity should be O($nd_{model}^2$)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXlE7WLdJkFR"
      },
      "source": [
        "4- [Extra credit] Modify your implementation above (by adding a block of code below) to make your model work as a encoder-decoder transformer instead of decoder-only model.  \n",
        "Explain only the required changes, don't just dumpt new the code.  \n",
        "However, when needed, provide concrete implmentation changes in form of short snippets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYlmotnGJkFR"
      },
      "outputs": [],
      "source": [
        "# Optional: Encoder-decoder modifications"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79c87f52018b4af48f1d8834aae4a930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_718f47b73c1d4d49a68b3b71add22c0e",
              "IPY_MODEL_3cccf3e7d6894118bc59e2c6e5890bad",
              "IPY_MODEL_5812354c13c143769cb64ef5dfd6ca5e"
            ],
            "layout": "IPY_MODEL_bb6977ef2a3246bbb288d2ec0f883108"
          }
        },
        "718f47b73c1d4d49a68b3b71add22c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b15943292a74cd4b98e882fcea2a687",
            "placeholder": "​",
            "style": "IPY_MODEL_ef5982e589ec495ca257ed3c319b3f10",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3cccf3e7d6894118bc59e2c6e5890bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cd02f9debaa4d788828fcfcd88fdb81",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d067a48333bb405ea4e04e9ddba0e4d7",
            "value": 26
          }
        },
        "5812354c13c143769cb64ef5dfd6ca5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae18816896f4e378540467c0c238b20",
            "placeholder": "​",
            "style": "IPY_MODEL_61853934e9564267b2fefb9c334c4960",
            "value": " 26.0/26.0 [00:00&lt;00:00, 488B/s]"
          }
        },
        "bb6977ef2a3246bbb288d2ec0f883108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b15943292a74cd4b98e882fcea2a687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5982e589ec495ca257ed3c319b3f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cd02f9debaa4d788828fcfcd88fdb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d067a48333bb405ea4e04e9ddba0e4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ae18816896f4e378540467c0c238b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61853934e9564267b2fefb9c334c4960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de82744b68cb488b942b7d169dc70c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f15e9aca036444b85f52c5a1c03b625",
              "IPY_MODEL_6f3e7c85dc544ace866ba318af08fdeb",
              "IPY_MODEL_86fae1252b174c34946a31497c16dddb"
            ],
            "layout": "IPY_MODEL_823319e59a5f44128c4aea1a8a818ffb"
          }
        },
        "5f15e9aca036444b85f52c5a1c03b625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da87436cbad442438ba56fc7cd3cca1c",
            "placeholder": "​",
            "style": "IPY_MODEL_71c0234cb9af4975bc35c3763bfce18a",
            "value": "vocab.json: 100%"
          }
        },
        "6f3e7c85dc544ace866ba318af08fdeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be60ff4cb63d499da7509740fb87f383",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43906c367edc4a4db96bcca1f468cf8f",
            "value": 1042301
          }
        },
        "86fae1252b174c34946a31497c16dddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd626ac057246719caa52ab28be9f39",
            "placeholder": "​",
            "style": "IPY_MODEL_c816fcd4f86441fb829aa3f62cba8a16",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 11.7MB/s]"
          }
        },
        "823319e59a5f44128c4aea1a8a818ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da87436cbad442438ba56fc7cd3cca1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71c0234cb9af4975bc35c3763bfce18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be60ff4cb63d499da7509740fb87f383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43906c367edc4a4db96bcca1f468cf8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efd626ac057246719caa52ab28be9f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c816fcd4f86441fb829aa3f62cba8a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d10c9f60745845bb8e9ae7effc107a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_502ec6ac60fc468fb286d3df2ce2bae1",
              "IPY_MODEL_b2424363ff0140b88d2a5cbd8889a5c4",
              "IPY_MODEL_d78ad36a305f4645a6100fe8e656bf7a"
            ],
            "layout": "IPY_MODEL_44226c88ba594504b2fe7e70917b6062"
          }
        },
        "502ec6ac60fc468fb286d3df2ce2bae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ac9566d091840a79e94baf9671b0ea2",
            "placeholder": "​",
            "style": "IPY_MODEL_8b328047aad64bfaa74058800447ee33",
            "value": "merges.txt: 100%"
          }
        },
        "b2424363ff0140b88d2a5cbd8889a5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130db72de3d14a2b9e0ebf812e117cf1",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e9ecfdacefe4f56b015c1e8ea4e81f4",
            "value": 456318
          }
        },
        "d78ad36a305f4645a6100fe8e656bf7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a83a7765a84fb69442e297384990f0",
            "placeholder": "​",
            "style": "IPY_MODEL_97f0f5d9bf2c4d479bc6fe9622a14467",
            "value": " 456k/456k [00:00&lt;00:00, 6.73MB/s]"
          }
        },
        "44226c88ba594504b2fe7e70917b6062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac9566d091840a79e94baf9671b0ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b328047aad64bfaa74058800447ee33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "130db72de3d14a2b9e0ebf812e117cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9ecfdacefe4f56b015c1e8ea4e81f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74a83a7765a84fb69442e297384990f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f0f5d9bf2c4d479bc6fe9622a14467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "362b9344725e4882bb2f45784ae104db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50edf0fdd855416f892bab6d4f26cf82",
              "IPY_MODEL_c8cd605e2e904ae195754db8e7759afb",
              "IPY_MODEL_dfcc9ad5dbce427ea86e0335b2243c68"
            ],
            "layout": "IPY_MODEL_6c0a8fb4f7fc4a1e9f7a6b7f79af570d"
          }
        },
        "50edf0fdd855416f892bab6d4f26cf82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fff74adf1f5403f8273ecc038207d14",
            "placeholder": "​",
            "style": "IPY_MODEL_e1f60b9b16a34576a005e9953d1ecb5b",
            "value": "tokenizer.json: 100%"
          }
        },
        "c8cd605e2e904ae195754db8e7759afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50088f265ae442ef8ce05cc9428ebdc5",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b56dcce689149e6aa64638991ed20a5",
            "value": 1355256
          }
        },
        "dfcc9ad5dbce427ea86e0335b2243c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d736cf2c91854dc9ad2c1a8aedac3059",
            "placeholder": "​",
            "style": "IPY_MODEL_4249bb2c1a004fb49f79dc657c28bd3f",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 15.6MB/s]"
          }
        },
        "6c0a8fb4f7fc4a1e9f7a6b7f79af570d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fff74adf1f5403f8273ecc038207d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1f60b9b16a34576a005e9953d1ecb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50088f265ae442ef8ce05cc9428ebdc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b56dcce689149e6aa64638991ed20a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d736cf2c91854dc9ad2c1a8aedac3059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4249bb2c1a004fb49f79dc657c28bd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b3b11aa6264d518f5705ca703647c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_537d4b0d6aee418db4231a985d3311ba",
              "IPY_MODEL_7141128b489c4b80bef17e1c74f44946",
              "IPY_MODEL_3842d3fee6ff4551ba5ef920419ccec6"
            ],
            "layout": "IPY_MODEL_54e91d1f2f4b4352952f629d9572145b"
          }
        },
        "537d4b0d6aee418db4231a985d3311ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_925768ba7d414416a87f3ac47700e50a",
            "placeholder": "​",
            "style": "IPY_MODEL_828be382d11146668c88979b6feb5623",
            "value": "config.json: 100%"
          }
        },
        "7141128b489c4b80bef17e1c74f44946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dee0cb346114487884b457efd0d1f494",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1600ec833262406c851058c7142e8e69",
            "value": 665
          }
        },
        "3842d3fee6ff4551ba5ef920419ccec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86dbbc556c8c442bb71f8d4620763d82",
            "placeholder": "​",
            "style": "IPY_MODEL_11f6cb720cd54ca6b34c2bfc40baefe6",
            "value": " 665/665 [00:00&lt;00:00, 13.3kB/s]"
          }
        },
        "54e91d1f2f4b4352952f629d9572145b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925768ba7d414416a87f3ac47700e50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "828be382d11146668c88979b6feb5623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dee0cb346114487884b457efd0d1f494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1600ec833262406c851058c7142e8e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86dbbc556c8c442bb71f8d4620763d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f6cb720cd54ca6b34c2bfc40baefe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}