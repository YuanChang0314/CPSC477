{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4G--JVIfw2E"
      },
      "source": [
        "# Sparse Word Representations Assignment (50 points)\n",
        "\n",
        "**Objective:** Implement and apply sparse word representations using co-occurrence counts and TF-IDF weighting on a small document collection.\n",
        "\n",
        "**Dataset:** We will use the 20 Newsgroups dataset, readily available in scikit-learn. We'll limit it to a subset of categories and a reduced number of documents to keep things manageable.\n",
        "\n",
        "```\n",
        "Instructions:\n",
        "Fill out the parts that are specified with\n",
        "% -- Your Implementation -- %\n",
        "```\n",
        "\n",
        "### 1. Data Loading and Preprocessing (5 points)\n",
        "\n",
        "Steps:\n",
        "- Load the 20 Newsgroups dataset using scikit-learn, but only select 4 categories of your choice.\n",
        "- Limit the dataset to the first 1000 documents.\n",
        "- Tokenize the documents into words. You can use a simple tokenizer like splitting on spaces and remove any punctuations.\n",
        "- Create a vocabulary of the 5000 most frequent words across all documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OLFntWSffw2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40715ec-2aee-47f7-b93a-e8f4a13ea867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "!pip install scikit-learn\n",
        "from typing import List, Set, Dict, Tuple, Union\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from math import log\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ljp7y8zkfw2G"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load the dataset with selected categories\n",
        "categories = ['talk.politics.misc', 'sci.med']\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ccu947tQfw2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faaa29ea-ebbe-4a58-b3f5-44f9157a5a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5000\n",
            "\n",
            "Sample vocabulary words:\n",
            "['protecting', 'classes', 'earth', 'ice', 'distilled', '52', 'spikes', 'not', 'property', 'applied']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Set\n",
        "\n",
        "def preprocess_text(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize and clean text by:\n",
        "    1. Converting to lowercase\n",
        "    2. Removing punctuation\n",
        "    3. Splitting on whitespace\n",
        "    4. Removing empty tokens\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return tokens\n",
        "\n",
        "# Get first 1000 documents\n",
        "documents = newsgroups_data.data[:2000]\n",
        "\n",
        "# Tokenize all documents\n",
        "tokenized_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "# Now write a fucntion to get the vocabulary\n",
        "def get_vocab(tokenized_docs: List[List[str]], vocab_size: int) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Create vocabulary from tokenized documents by:\n",
        "    1. Counting word frequencies across all documents\n",
        "    2. Taking the `vocab_size` most common words\n",
        "\n",
        "    Args:\n",
        "        tokenized_docs: List of documents, where each document is a list of tokens\n",
        "\n",
        "    Returns:\n",
        "        Set of vocabulary words\n",
        "    \"\"\"\n",
        "    all_tokens = []\n",
        "    for doc in tokenized_docs:\n",
        "        all_tokens.extend(doc)\n",
        "    counter = Counter(all_tokens)\n",
        "    most_common_words = counter.most_common(vocab_size)\n",
        "    vocab = set([word for word, _ in most_common_words])\n",
        "    return vocab\n",
        "\n",
        "vocab = get_vocab(tokenized_docs, vocab_size=5000)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(\"\\nSample vocabulary words:\")\n",
        "print(list(vocab)[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL-VE_ckfw2G"
      },
      "source": [
        "## 2. Computing the co-occurrence matrix (10 points)\n",
        "\n",
        "Assume co-occurrence means if a word appears in the same document and within a window of size 10 of another word. Compute the co-occurrence matrix based on raw frequencies.\n",
        "\n",
        "For example in the sentence: \"the hungry cat ate the small fish near the blue lake\"\n",
        "\n",
        "With window size 6, the co-occurrences for the word \"cat\":\n",
        "\n",
        "Left context (within 2 words): \"the\", \"hungry\"\n",
        "Right context (within 3 words): \"ate\", \"the\", \"small\"\n",
        "So \"cat\" co-occurs once each with: hungry, ate, small\n",
        "And it co-occurs twice with the word: \"the\"\n",
        "\n",
        "Words outside the window like \"fish\", \"near\", \"blue\", \"lake\" are not counted as co-occurrences with \"cat\" in this case.\n",
        "\n",
        "Complete the following steps:\n",
        "\n",
        "- Construct a co-occurrence matrix. This matrix will have dimensions `vocab_size` x `vocab_size` (5000 x 5000 in our case).\n",
        "- For each document, iterate through its words. For each word, increment the corresponding entries in the co-occurrence matrix for its neighboring words within a specified window size (e.g., a window of 5 words to the left and right).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pzwjpcGIfw2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9080ffc7-59f6-42c4-d1b5-b22f3dbcbe16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix shape: (5000, 5000)\n",
            "\n",
            "Sample co-occurrence matrix:\n",
            "[[0 0 0 1 1]\n",
            " [0 4 1 0 0]\n",
            " [0 1 0 0 0]\n",
            " [1 0 0 8 3]\n",
            " [1 0 0 3 2]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_cooccurrence_matrix(tokenized_docs: List[List[str]], vocab: Set[str], window_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute word co-occurrence matrix from tokenized documents using a sliding window approach.\n",
        "\n",
        "    Args:\n",
        "        tokenized_docs (List[List[str]]): List of documents where each document is a list of tokens\n",
        "        vocab (Set[str]): Set of vocabulary words to consider\n",
        "        window_size (int): Number of words to consider\n",
        "            if odd, there are window_size//2 words on each side of the center word\n",
        "            if even, there are window_size//2 words on the left and window_size//2 - 1 words on the right\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Co-occurrence matrix of shape (vocab_size, vocab_size) where entry [i,j]\n",
        "                   represents how many times word i appears within the window of word j\n",
        "    \"\"\"\n",
        "    vocab_list = sorted(list(vocab))\n",
        "    vocab_size = len(vocab_list)\n",
        "    word2idx = {word: i for i, word in enumerate(vocab_list)}\n",
        "    cooc_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int64)\n",
        "    for doc in tokenized_docs:\n",
        "      doc_length = len(doc)\n",
        "      for i, word in enumerate(doc):\n",
        "        if word not in vocab:\n",
        "          continue\n",
        "        word_idx = word2idx[word]\n",
        "        left_context = max(0, i - window_size//2)\n",
        "        right_context = min(doc_length, i + window_size//2 + 1)\n",
        "        for j in range(left_context, right_context):\n",
        "          if j == i:\n",
        "            continue\n",
        "          context_word = doc[j]\n",
        "          if context_word in vocab:\n",
        "            context_idx = word2idx[context_word]\n",
        "            cooc_matrix[word_idx, context_idx] += 1\n",
        "    return cooc_matrix\n",
        "\n",
        "\n",
        "cooccurrence_matrix = compute_cooccurrence_matrix(tokenized_docs, vocab, window_size=10)\n",
        "print(f\"Co-occurrence matrix shape: {cooccurrence_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHWBesPfw2G"
      },
      "source": [
        "## 3. Application: Word Similarity (10 points)\n",
        "\n",
        "- Implement a function to calculate the cosine similarity between two *word* vectors.\n",
        "- For the following words, find the 5 most similar words to each of your chosen words based on their raw co-occurrence representations using the cosine similarity function.\n",
        "[concerned, lone, matthew]\n",
        "- Print the chosen words and their 5 most similar words, along with their similarity scores.\n",
        "\n",
        "Recall that `Cosine Similarity(A, B) = (A . B) / (||A|| * ||B||)`\n",
        "where `.` denotes the dot product, and `||A||` is the magnitude (Euclidean norm) of vector A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "29ipOjDofw2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9881ca-1f4a-4882-f59b-535d80fdf094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('concerned', [('animals', 0.6514739218413861), ('europe', 0.628410287203555), ('added', 0.6119238072666792), ('favor', 0.6091273377913761), ('36', 0.6072338280733376)]), ('lone', [('credibility', 0.8162868810391121), ('sitting', 0.8124927632356572), ('debunking', 0.8120074707937157), ('epidemic', 0.8091645459422537), ('meal', 0.8085538590593634)]), ('matthew', [('relation', 0.8326262912298619), ('slow', 0.8324409724104836), ('located', 0.8282410234341334), ('grants', 0.8272675439506562), ('formerly', 0.8237826324581541)])]\n"
          ]
        }
      ],
      "source": [
        "words = ['concerned', 'lone', 'matthew']\n",
        "\n",
        "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
        "        return 0\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "def find_most_similar_words(words: List[str], cooccurrence_matrix: np.ndarray) -> List[Tuple[str, List[Tuple[str, float]]]]:\n",
        "    \"\"\"\n",
        "    Find the 5 most similar words to each input word based on their co-occurrence representations.\n",
        "\n",
        "    Args:\n",
        "        words (List[str]): List of words to find similar words for\n",
        "        cooccurrence_matrix (np.ndarray)\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, List[Tuple[str, float]]]]: For each input word, returns a tuple containing:\n",
        "            - The input word\n",
        "            - A list of 5 tuples, each containing:\n",
        "                - A similar word\n",
        "                - The cosine similarity score between the input and similar word\n",
        "\n",
        "    Example:\n",
        "        >>> result = find_most_similar_words(['cat'], cooccurrence_matrix)\n",
        "        >>> result\n",
        "        [('cat', [('dog', 0.85), ('kitten', 0.76), ('pet', 0.72), ...])]\n",
        "    \"\"\"\n",
        "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx2word = {i: word for word, i in word2idx.items()}\n",
        "    result = []\n",
        "    for word in words:\n",
        "        if word not in vocab:\n",
        "            continue\n",
        "        word_idx = word2idx[word]\n",
        "        word_vector = cooccurrence_matrix[word_idx]\n",
        "        similarities = [(idx2word[i], cosine_similarity(word_vector, cooccurrence_matrix[i])) for i in range(len(vocab)) if i != word_idx]\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        result.append((word, similarities[:5]))\n",
        "    return result\n",
        "\n",
        "\n",
        "print(find_most_similar_words(words, cooccurrence_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcGhQ9yGfw2G"
      },
      "source": [
        "## 4. TF-IDF weighting (10 points)\n",
        "\n",
        "In this part you will implement TF-IDF weighting, which is a numerical statistic that reflects how important a word is to a document in a collection or corpus. TF-IDF consists of two components:\n",
        "1. Term Frequency (TF): How frequently a word appears in a document  \n",
        "2. Inverse Document Frequency (IDF): How unique or rare the word is across all documents  \n",
        "\n",
        "Assume we have a term-document matrix of co-occurrences.\n",
        "Then we can use TF-IDF to reweight the raw frequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXKfPPlIfw2G"
      },
      "source": [
        "First compute the TF. Recall that TF is:\n",
        "\n",
        "tf(t,d) = 1 + log(count(t,d)) if term t appears in document d  \n",
        "tf(t,d) = 0 if term t doesn't appear in document d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pj2Oz8unfw2H"
      },
      "outputs": [],
      "source": [
        "def compute_tf(document: Union[str, List[str]], vocab: Set[str]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute term frequencies in a document.\n",
        "\n",
        "    \"\"\"\n",
        "    counter = Counter(document)\n",
        "    tf_dict = {}\n",
        "    for word in vocab:\n",
        "        if word in counter:\n",
        "            tf_dict[word] = 1 + log(counter[word])\n",
        "        else:\n",
        "            tf_dict[word] = 0\n",
        "    return tf_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkUsbBXDfw2H"
      },
      "source": [
        "### Compute inverse document frequency\n",
        "Implement a function to calculate document frequencies across a corpus.\n",
        "Recall that inverse document frequency is:\n",
        "\n",
        "log ( N / df ), where:\n",
        "\n",
        "- **N** is the total number of documents in the corpus.\n",
        "- **df** is the document frequency, i.e., the number of documents in which the term appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MrYoAYRGfw2H"
      },
      "outputs": [],
      "source": [
        "def compute_idf(documents: Union[List[str], List[List[str]]], vocab: Set[str]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute inverse document frequency for all terms in the corpus.\n",
        "    Returns a dictionary mapping terms to their IDF scores.\n",
        "    \"\"\"\n",
        "    N = len(documents)\n",
        "    df = {}\n",
        "    for word in vocab:\n",
        "        df[word] = sum(1 for doc in documents if word in doc)\n",
        "    word2idf = {word: log(N / (1 + df[word])) for word in vocab}\n",
        "    return word2idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjFC17B-fw2H"
      },
      "source": [
        "### Caclculate the co-occurrence matrix with TF-IDF weights.\n",
        "\n",
        "Now we want to calculate TF-IDF scores for each term in each document. We'll combine the term frequency (TF) and inverse document frequency (IDF) scores we computed earlier to get the final TF-IDF weights.  \n",
        "This will give us a measure of how important each word is to a document in the context of the entire corpus.\n",
        "\n",
        "Recall that the TF-IDF score is calculated as:  \n",
        "`tf-idf = tf * idf`  \n",
        "\n",
        "Where:  \n",
        "- tf is the term frequency (number of times term appears in document)  \n",
        "- idf is the inverse document frequency `(log(N/df))`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FUGwrQ25fw2H"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf(tokenized_docs: List[List[str]], vocab: Set[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute TF-IDF (Term Frequency-Inverse Document Frequency) matrix for a collection of documents.\n",
        "\n",
        "    The TF-IDF score is calculated as:\n",
        "    tf-idf(t,d) = tf(t,d) * idf(t)\n",
        "\n",
        "    Args:\n",
        "        tokenized_docs (List[List[str]]): List of documents, where each document is a list of tokens\n",
        "        vocab (Set[str]): Set of vocabulary words to consider\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: TF-IDF matrix of shape (vocab_size, num_documents) where:\n",
        "            - Each row represents a term from the vocabulary\n",
        "            - Each column represents a document\n",
        "            - Entry [i,j] represents the TF-IDF score of term i in document j\n",
        "\n",
        "    Note:\n",
        "        - IDF calculation includes +1 smoothing to avoid division by zero\n",
        "        - TF calculation uses log normalization: 1 + log(term_count)\n",
        "    \"\"\"\n",
        "    word2idf = compute_idf(tokenized_docs, vocab)\n",
        "    tfidf_matrix = np.zeros((len(vocab), len(tokenized_docs)))\n",
        "    for i, doc in enumerate(tokenized_docs):\n",
        "        tf_dict = compute_tf(doc, vocab)\n",
        "        for j, word in enumerate(vocab):\n",
        "            tfidf_matrix[j, i] = tf_dict[word] * word2idf[word]\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "word_document_matrix = compute_tfidf(tokenized_docs, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prjtnDpkfw2H"
      },
      "source": [
        "### Application in search (10 points)\n",
        "\n",
        "Now we will use TF-IDF representations to implement a simple search function.\n",
        "In this function we match the most similar documents to a given query.\n",
        "The query itself should be converted to a TF-IDF vector as above.\n",
        "\n",
        "Implement the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sWDVvyjEfw2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be684ab3-6bef-406c-c41f-5f0c9bed3ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most relevant documents for query: 'medical treatment for heart disease'\n",
            "\n",
            "Document 225 (score: 0.217):\n",
            "im curious why you think that particular adjective is important...\n",
            "\n",
            "Document 32 (score: 0.172):\n",
            "its not a cycle free helium will escape from the atmosphere due to its high velocity it wont be prac...\n",
            "\n",
            "Document 391 (score: 0.147):\n",
            "gordon banks this certainly describes my situation perfectly for me there is a constant dynamic betw...\n",
            "\n",
            "Document 146 (score: 0.137):\n",
            "what kind of brainless clod doesnt understand the difference between a proposed bill blocked in cong...\n",
            "\n",
            "Document 694 (score: 0.137):\n",
            "hey i wasnt the one dancing and singing on jan 20 now was i i was roundly ridiculed for my predictio...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-634e602ebb04>:54: RuntimeWarning: invalid value encountered in divide\n",
            "  similarities = dot_products / (query_norm * doc_norms)\n"
          ]
        }
      ],
      "source": [
        "def search_documents(query: str, tokenized_docs: List[List[str]], tfidf_matrix: np.ndarray, vocab: Set[str]) -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Search for documents most relevant to a query using TF-IDF representations.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query string\n",
        "        tokenized_docs (List[List[str]]): List of documents where each document is a list of tokens\n",
        "        tfidf_matrix (np.ndarray): TF-IDF matrix of shape (vocab_size, num_documents)\n",
        "        vocab (Set[str]): Vocabulary set used for TF-IDF computation\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[int, float]]: List of (document_index, similarity_score) tuples,\n",
        "            sorted by relevance (highest similarity first)\n",
        "    \"\"\"\n",
        "    vocab_list = sorted(vocab)\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocab_list)}\n",
        "    query_tokens = query.lower().split()\n",
        "\n",
        "    N = len(tokenized_docs)\n",
        "    df_counts = {word: 0 for word in vocab_list}\n",
        "    for doc in tokenized_docs:\n",
        "        unique_tokens = set(token.lower() for token in doc)\n",
        "        for token in unique_tokens:\n",
        "            if token in df_counts:\n",
        "                df_counts[token] += 1\n",
        "    idf = {word: np.log((N + 1) / (df_counts[word] + 1)) + 1 for word in vocab_list}\n",
        "    query_tf = {}\n",
        "    for token in query_tokens:\n",
        "        if token in word_to_index:\n",
        "            query_tf[token] = query_tf.get(token, 0) + 1\n",
        "    query_vec = np.zeros(len(vocab_list))\n",
        "    for token, count in query_tf.items():\n",
        "        idx = word_to_index[token]\n",
        "        query_vec[idx] = count * idf[token]\n",
        "    dot_products = np.dot(query_vec, tfidf_matrix)\n",
        "    query_norm = np.linalg.norm(query_vec)\n",
        "    doc_norms = np.linalg.norm(tfidf_matrix, axis=0)\n",
        "    if query_norm == 0:\n",
        "        similarities = np.zeros(tfidf_matrix.shape[1])\n",
        "    else:\n",
        "        similarities = dot_products / (query_norm * doc_norms)\n",
        "        similarities = np.nan_to_num(similarities)\n",
        "    results = list(enumerate(similarities))\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "query = \"medical treatment for heart disease\"\n",
        "results = search_documents(query, tokenized_docs, word_document_matrix, vocab)\n",
        "\n",
        "print(f\"Top 5 most relevant documents for query: '{query}'\\n\")\n",
        "for doc_idx, score in results[:5]:\n",
        "    # Print first 100 characters of each document\n",
        "    preview = ' '.join(tokenized_docs[doc_idx])[:100] + \"...\"\n",
        "    print(f\"Document {doc_idx} (score: {score:.3f}):\")\n",
        "    print(f\"{preview}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOBuomGBfw2H"
      },
      "source": [
        "## 5. Reflection questions (10 points)\n",
        "\n",
        "Now please answer the following reflection questions:\n",
        "\n",
        "1- How do the most similar words differ when using raw co-occurrence counts versus TF-IDF weighted representations? What might explain these differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zUO6l5cfw2H"
      },
      "source": [
        "# Your answer here\n",
        "    The similar word genearate by raw co-occurrence is affected by the high frequence word around that word. So the similar word might not have a strong association with the word. TF-IDF weighting adjusts these counts by reducing the impact of frequence, instead weighting more the importance in the whole document that carry more unique or context-specific information.\n",
        "    Which makes the TF-IDF generating more related word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcM1gm7lfw2H"
      },
      "source": [
        "2. What are the limitations of using a fixed window size for co-occurrence counting? How might this impact the quality of word similarities for different types of words (e.g., nouns vs. verbs)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaPP9c1Tfw2H"
      },
      "source": [
        "# Your answer here\n",
        "    The first limitation is that the important words might be outside the window sized, and large windowsize might include too much useless information.\n",
        "    Also different word might need different window sized to fetch the enough informaion. For example the verb and adj might have different effected window sized\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGCorCyfw2H"
      },
      "source": [
        "3- How does the choice of preprocessing steps (like removing punctuation, converting to lowercase, etc.) affect the resulting word representations? Can you think of cases where preserving some of this information might be beneficial?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XhIj4gDfw2H"
      },
      "source": [
        "# Your answer here\n",
        "    Preprocessing steps can effectively remove repeat words such as a same word in the begining of a sentence. This can help the model learn more meaningful information. However sometimes the punctuation can be useful for example ! can giving the same word different emotion thus influence the meaning of a sentence."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}