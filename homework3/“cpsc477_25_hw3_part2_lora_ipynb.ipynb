{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wFci-4MprSFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3 - Part 2: Implementing LoRA From Scratch (70 points)\n",
        "\n",
        "\n",
        "By the end of this assignment, you will:\n",
        "\n",
        "- Understand the theory behind Low-Rank Adaptation (LoRA)\n",
        "- Implement a LoRA layer from scratch using PyTorch\n",
        "- Apply LoRA to modify a pre-trained RoBERTa model\n",
        "- Train a LoRA model and compute accuracy\n",
        "\n",
        "## Background\n",
        "LLMs have become increasingly powerful but also increasingly parameter-heavy, making fine-tuning computationally expensive.\n",
        "\n",
        "# Instructions\n",
        "\n",
        "Please make sure you look carefully at all codes.\n",
        "The parts that you need to do are either annotated with `TODO` or `<<< YOUR ANSWER/CODE HERE >>>` makers\n",
        "\n",
        "Submit the notebook as a .ipynb file through GradeScope.\n",
        "\n",
        "Make sure that the notebook is running without any errors before submission. Remove any unnecessary outputs or additional `print` or debugging statements that you put in the code before submission.\n",
        "\n",
        "### Write your name and NetID below.\n",
        "\n",
        "**Name:**    Yuan Chang\n",
        "\n",
        "**NetID:**   yc2238_\n",
        "\n",
        "### Acknowledgement\n",
        "\n",
        "The assignment is designed by TA Yilun Zhao, with help and guidance from Arman Cohan."
      ],
      "metadata": {
        "id": "-EPSRBKlqkCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Question: What exaclty makes full fine-tuning expensive?  (3 points)"
      ],
      "metadata": {
        "id": "2OqmBgOjqyNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: You answer to the above question here. Replace the Raise statement\n",
        "\n",
        "your_answer = \"\"\"because the full fine tuning requires the model to recaculate all the parameter and do the backpropagation on every parameter which makes the cost huge. Also it will\n",
        "cost a lot of storage resourse to load all the parameter.\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "if your_answer == \"\":\n",
        "  raise NotImplementedError()"
      ],
      "metadata": {
        "id": "LKlOu6Jkq-Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Part 1: Implementing LoRA Layer (15 points)\n",
        "\n",
        "Implement a LoRA layer from scratch in PyTorch. Your implementation should:\n",
        "\n",
        "Create a class LoRALayer that:\n",
        "\n",
        "- Takes input dimension, output dimension, rank, and alpha scaling factor as parameters\n",
        "- Initializes matrices A and B using appropriate initialization schemes\n",
        "- Implements the forward pass that computes the low-rank adaptation\n",
        "\n",
        "\n",
        "Create a class LinearWithLoRA that:\n",
        "\n",
        "- Wraps around a pre-existing nn.Linear layer\n",
        "- Adds the LoRA adaptation to the original linear transformation\n",
        "- Implements the forward pass to combine outputs from the original linear layer and the LoRA adaptation\n",
        "\n",
        "We will provide you with starter code below:"
      ],
      "metadata": {
        "id": "COScNFKGrE9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "VkywAj-auA70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "pL9QMlXdt5k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 1: Implement the LoRA Layer ---\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # --- TODO 1.1: Initialize LoRA Matrices A and B ---\n",
        "        # Initialize matrix 'A' (shape: in_dim x rank) with values drawn from a normal\n",
        "        # distribution. A common practice is to scale the standard deviation.\n",
        "        # Hint: Use nn.Parameter() to make these trainable tensors.\n",
        "        # Hint: std_dev = 1 / torch.sqrt(torch.tensor(rank).float()) might be useful.\n",
        "\n",
        "        # Initialize matrix 'B' (shape: rank x out_dim) with zeros.\n",
        "        # Why initialize B with zeros? What effect does this have initially? (See Question 1.3)\n",
        "\n",
        "        # Replace the following lines with your implementation:\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev) # <<< YOUR CODE HERE FOR A >>>\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim)) # <<< YOUR CODE HERE FOR B >>>\n",
        "        # --- End TODO 1.1 ---\n",
        "\n",
        "        # Safety check (optional but good practice)\n",
        "        if self.A is None or self.B is None:\n",
        "             raise NotImplementedError(\"TODO 1.1: Initialize self.A and self.B\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- TODO 1.2: Implement the LoRA Forward Pass ---\n",
        "        # Calculate the LoRA adjustment: alpha * (x @ A @ B)\n",
        "        # Note: The scaling factor `alpha` is often applied here, though some\n",
        "        # implementations might incorporate it differently (e.g., merged with A or B).\n",
        "        # I encourage you to read the LoRA paper section 4 to better understand\n",
        "        # what is exactly going on: https://arxiv.org/pdf/2106.09685\n",
        "        # This implementation applies it during the forward pass.\n",
        "\n",
        "        # Replace the following line with your implementation:\n",
        "        lora_output = x @ self.A @ self.B # <<< YOUR CODE HERE >>>\n",
        "        lora_output = self.alpha * lora_output\n",
        "        # --- End TODO 1.2 ---\n",
        "\n",
        "        if lora_output is None: # Safety check\n",
        "             raise NotImplementedError(\"TODO 1.2: Implement the forward pass\")\n",
        "        return lora_output\n",
        "\n",
        "# --- Task 2: Implement the Wrapper Layer ---\n",
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    def __init__(self, linear: nn.Linear, rank: int, alpha: int):\n",
        "        super().__init__()\n",
        "        self.linear = linear # The original frozen linear layer\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Store original dimensions for clarity (optional)\n",
        "        self.in_features = linear.in_features\n",
        "        self.out_features = linear.out_features\n",
        "\n",
        "        # --- TODO 2.1: Create the LoRALayer instance ---\n",
        "        # Instantiate the `LoRALayer` you defined above.\n",
        "        # It needs the input/output dimensions of the original `linear` layer,\n",
        "        # and the `rank` and `alpha` values passed to this constructor.\n",
        "\n",
        "        # Replace the following line with your implementation:\n",
        "        self.lora = LoRALayer(\n",
        "            in_dim=self.in_features,\n",
        "            out_dim=self.out_features,\n",
        "            rank=rank,\n",
        "            alpha=alpha\n",
        "        ) # <<< YOUR CODE HERE: Instantiate LoRALayer >>>\n",
        "        # --- End TODO 2.1 ---\n",
        "\n",
        "        # Safety check\n",
        "        if self.lora is None:\n",
        "             raise NotImplementedError(\"TODO 2.1: Instantiate self.lora\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- TODO 2.2: Combine Original and LoRA Outputs ---\n",
        "        # The forward pass should compute the output of the original linear layer\n",
        "        # and add the output of the LoRA layer to it.\n",
        "        # Remember: The original `self.linear` layer's parameters should be frozen.\n",
        "        # Only the parameters inside `self.lora` (A and B) should be trainable.\n",
        "\n",
        "        # Replace the following line with your implementation:\n",
        "        combined_output = self.linear(x) + self.lora(x) # <<< YOUR CODE HERE >>>\n",
        "        # --- End TODO 2.2 ---\n",
        "\n",
        "        if combined_output is None: # Safety check\n",
        "             raise NotImplementedError(\"TODO 2.2: Implement the combined forward pass\")\n",
        "        return combined_output\n"
      ],
      "metadata": {
        "id": "M6tY3EFErmpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Modifying RoBERTa with LoRA (20 points)\n",
        "\n",
        "In this part we will apply LoRA to an existing pretrained language model, RoBERTa. We will use the implementation of RoBERTa on the huggingface library.\n",
        "\n",
        "Load a pre-trained RoBERTa model for sequence classification:"
      ],
      "metadata": {
        "id": "igDEsNl2rwO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", num_labels=2)"
      ],
      "metadata": {
        "id": "hbR52G9Xrt-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will do the following next:\n",
        "\n",
        "1- Freeze all model parameters  \n",
        "\n",
        "2- Replace specific linear layers with your LoRA-enhanced layers. Consider:\n",
        "* Query matrices in the attention mechanism\n",
        "* Key matrices\n",
        "* Value matrices\n",
        "* Output projection matrices\n",
        "* Feed-forward layers\n",
        "* Classification head\n",
        "\n"
      ],
      "metadata": {
        "id": "kCtVhJPmr-PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 1: Apply LoRA to RoBERTa ---\n",
        "def apply_lora_to_roberta(model: nn.Module, lora_r: int = 8, lora_alpha: int = 16):\n",
        "    \"\"\"\n",
        "    Modifies a RoBERTa model (in-place) by replacing specific nn.Linear layers\n",
        "    with LinearWithLoRA wrapper layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The RoBERTa model instance.\n",
        "        lora_r (int): The rank for LoRA matrices.\n",
        "        lora_alpha (int): The scaling factor for LoRA.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The modified model.\n",
        "    \"\"\"\n",
        "    # --- Step 1.1: Freeze Original Parameters (Already Done) ---\n",
        "    print(\"Freezing original model parameters...\")\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"Original parameters frozen.\")\n",
        "\n",
        "    # --- Step 1.2: Create a Partial Constructor ---\n",
        "    # `partial` pre-fills arguments for a function/constructor. Here, it creates\n",
        "    # a shortcut `assign_lora(linear_layer)` that is equivalent to calling\n",
        "    # `LinearWithLoRA(linear_layer, rank=lora_r, alpha=lora_alpha)`.\n",
        "    assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha=lora_alpha)\n",
        "    print(f\"Prepared LoRA wrapper with r={lora_r}, alpha={lora_alpha}\")\n",
        "\n",
        "    # --- TODO 1.3: Replace Target Layers ---\n",
        "    # Replace the specified `nn.Linear` layers in the RoBERTa model\n",
        "    # with their `LinearWithLoRA` wrapped versions using the `assign_lora` helper.\n",
        "    # The code explicitly targets layers by their attribute path.\n",
        "\n",
        "    print(\"Applying LoRA wrappers to target layers...\")\n",
        "    try:\n",
        "        # Iterate through each transformer layer in the RoBERTa encoder\n",
        "        for layer in model.roberta.encoder.layer:\n",
        "            # --- Fill in the replacement lines below ---\n",
        "            # hint: try to investigate the `layer` object to see what is its structure\n",
        "            # hint: you can add a breakpoint by addding a \"import pdb; pdb.set_trace()\" line\n",
        "            # and then see how the `layer` object looks like\n",
        "            # for example the Query layer in Roberta is stored in the\n",
        "            # layer.attention.self.query\n",
        "            # Now you will apply LoRA to each component\n",
        "\n",
        "            layer.attention.self.query = assign_lora(layer.attention.self.query) # <<< YOUR CODE HERE >>>\n",
        "            layer.attention.self.key = assign_lora(layer.attention.self.key)   # <<< YOUR CODE HERE >>>\n",
        "            layer.attention.self.value = assign_lora(layer.attention.self.value) # <<< YOUR CODE HERE >>>\n",
        "\n",
        "            # Also target dense layers in attention output, intermediate FF, and final output\n",
        "            layer.attention.output.dense = assign_lora(layer.attention.output.dense) # <<< YOUR CODE HERE >>>\n",
        "            layer.intermediate.dense = assign_lora(layer.intermediate.dense)     # <<< YOUR CODE HERE >>>\n",
        "            layer.output.dense = assign_lora(layer.output.dense)           # <<< YOUR CODE HERE >>>\n",
        "\n",
        "        # Also target layers in the final classifier head\n",
        "        # Check if the classifier structure matches RoBERTaForSequenceClassification\n",
        "        if hasattr(model, 'classifier') and hasattr(model.classifier, 'dense') and hasattr(model.classifier, 'out_proj'):\n",
        "             model.classifier.dense = assign_lora(model.classifier.dense)      # <<< YOUR CODE HERE >>>\n",
        "             model.classifier.out_proj = assign_lora(model.classifier.out_proj)   # <<< YOUR CODE HERE >>>\n",
        "        else:\n",
        "             print(\"Warning: Classifier structure not as expected. Skipping classifier LoRA application.\")\n",
        "        # --- End TODO 1.3 ---\n",
        "\n",
        "        print(\"Target layers replaced.\")\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"\\nERROR: An AttributeError occurred during layer replacement: {e}\")\n",
        "        print(\"This might happen if the model structure is different than expected (e.g., not RoBERTa).\")\n",
        "        print(\"Please check the model architecture and the attribute paths used for replacement.\")\n",
        "        raise # Re-raise the error after printing info\n",
        "\n",
        "    # --- Step 1.4: Verify Trainable Parameters (Already Done) ---\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTrainable params (LoRA A/B matrices): {trainable_params:,}\")\n",
        "    print(f\"Total params in model: {total_params:,}\")\n",
        "    if total_params > 0:\n",
        "      print(f\"Trainable parameter ratio: {trainable_params / total_params:.4%}\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "T624Nn6Ar7i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Fine-tuning and Evaluation (20 points)\n",
        "\n",
        "- Fine-tune your LoRA-enhanced RoBERTa model on the `SST-2` sentiment classification dataset.\n",
        "\n",
        "- Evaluate the model's performance on a validation set\n",
        "- Compare your LoRA fine-tuning with full finetuning\n"
      ],
      "metadata": {
        "id": "gh070afdsVxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sst2_data():\n",
        "    \"\"\"\n",
        "    Load and prepare SST-2 dataset for sentiment classification\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, eval_dataset: Dataset objects for training and evaluation\n",
        "    \"\"\"\n",
        "    # Load SST-2 dataset\n",
        "    dataset = load_dataset(\"glue\", \"sst2\")\n",
        "    train_dataset = dataset[\"train\"].select(range(10_000)) # only 10K examples out of 67K\n",
        "    eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    print(\"Tokenizer loaded\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # TASK 1: Take a look at the dataset.map function in the huggingface library\n",
        "    # Explain what is this function does\n",
        "    # https://huggingface.co/docs/datasets/en/process#map\n",
        "    # Then inspect the format of the `train_dataset` and explain what data structure it is and what fields/attributes it contains\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # data type and format\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "5fpiRDTYs0UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: write your answer for TASK 1 in your_answer_task1 below\n",
        "\n",
        "your_answer_task1 = \"\"\"map function create a transformation that applies a user defined function to each example. The data structure of train_dataset is\n",
        " object backed by an Arrow table, with columns accessible as Python lists or NumPy/PyTorch tensors\n",
        "\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "if your_answer_task1 == \"\":\n",
        "  raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "auYUvqiitodG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export WANDB_DISABLED=true"
      ],
      "metadata": {
        "id": "UdNkPa3cIQbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Tuple of predictions and labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # TASK 2: Explain this function in the next cell\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}"
      ],
      "metadata": {
        "id": "8lfTOx7YrgVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  Explain exactly what the `compute_metrics` function is doing (line by line)\n",
        "# you answer should be 3 lines, explaining each of the lines in the fucntion\n",
        "# write your answer in your_answer\n",
        "\n",
        "your_answer_task2 = \"\"\"First line is using huggingface to get a performance tuple which has the format raw model outputs, true labels)\n",
        "The second line is to merge the class-dimension into a single predicted label per example.\n",
        "The third line is to return the accuracy score of the prediction\"\"\"   # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "if your_answer_task2 == \"\":\n",
        "  raise NotImplementedError()\n",
        "elif len(your_answer_task2.split(\"\\n\")) != 3:\n",
        "  raise ValueError(\"Your answer should be 3 lines\")"
      ],
      "metadata": {
        "id": "wh3al9x7rjXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the following block of code carefully\n",
        "# then answer the questions in the next cell\n",
        "# corresponding to each of the TASKS\n",
        "\n",
        "def train_and_evaluate(model, train_dataset, eval_dataset):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_dataset: Dataset for training\n",
        "        eval_dataset: Dataset for evaluation\n",
        "\n",
        "    Returns:\n",
        "        Trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Define training arguments (this is needed by the Huggingface Trainer)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        disable_tqdm=False,\n",
        "        report_to=[\"none\"],\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    # TASK 3: Take a look at the implementation of the Trainer class\n",
        "    # and answer the questions regarding the TASK 2 in the next cell\n",
        "    # here is the documentation: https://huggingface.co/docs/transformers/en/main_classes/trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Task 4: Describe what happens conceptually when trainer.train() is called.\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(\"./best_checkpoint\")\n",
        "\n",
        "    # Task 5: what happens when you call trainer.evaluate()?\n",
        "    eval_results = trainer.evaluate()\n",
        "    return model, eval_results\n",
        "\n",
        "\n",
        "# 1. Load a base model\n",
        "original_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "print(\"Original model loaded.\")\n",
        "\n",
        "# 2. Apply LoRA modifications (example function you provided)\n",
        "lora_model = apply_lora_to_roberta(original_model, lora_r=8, lora_alpha=16)\n",
        "print(\"LoRA model created.\")\n",
        "# 3. Prepare your datasets (example function you provided)\n",
        "train_dataset, eval_dataset = prepare_sst2_data()\n",
        "print(\"Datasets prepared.\")\n",
        "\n",
        "# 4. Train and evaluate, saving the best checkpoint, it will take around 25 mins on a single T4 GPU\n",
        "trained_lora_model, eval_results = train_and_evaluate(lora_model, train_dataset, eval_dataset)\n",
        "print(\"Eval Results:\", eval_results)\n"
      ],
      "metadata": {
        "id": "2UIwwcEvvl6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  please add your answers to the TASKS defined in the above cell\n",
        "\n",
        "your_answer_task3 = \"\"\"Trainer is to Encapsulates the entire train/eval loop. It Logs training metrics at logging_steps and Saves model+optimizer state per save_strategy\n",
        "and aggregates, logs, and returns all requested metrics without extra user code.\"\"\"   # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "your_answer_task4 = \"\"\"it builds optimizer and scheduler from training_args, for each epoch and each batch it compute the loss and forward pass to compute logits.\n",
        "For every logging_steps, logs training loss, lr, etc\"\"\"   # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "your_answer_task5 = \"\"\"It switch to evaluation mode where it disables gradient computation and iterates over all eval batches, computing logits and collecting true labels\"\"\"   # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "if your_answer_task3 == \"\":\n",
        "  raise NotImplementedError()\n",
        "if your_answer_task4 == \"\":\n",
        "  raise NotImplementedError()\n",
        "if your_answer_task5 == \"\":\n",
        "  raise NotImplementedError()"
      ],
      "metadata": {
        "id": "twcFDRqQu2CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Load the best checkpoint and re-evaluate\n",
        "#    This demonstrates how you can load the checkpoint for inference or other tasks.\n",
        "\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "def load_and_evaluate(checkpoint_path, eval_dataset, lora_r=8, lora_alpha=16):\n",
        "    \"\"\"\n",
        "    Load a LoRA-trained model checkpoint (saved as model.safetensors)\n",
        "    and evaluate on eval_dataset.\n",
        "    \"\"\"\n",
        "    # 1. Load the base model\n",
        "    loaded_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "\n",
        "    # 2. Re-apply LoRA modifications\n",
        "    loaded_model = apply_lora_to_roberta(loaded_model, lora_r, lora_alpha)\n",
        "\n",
        "    # 3. Load safetensors state dict (instead of pytorch_model.bin)\n",
        "    state_dict_path = os.path.join(checkpoint_path, \"model.safetensors\")\n",
        "    state_dict = load_file(state_dict_path)  # safetensors load\n",
        "    loaded_model.load_state_dict(state_dict)\n",
        "\n",
        "    # 4. Evaluate with Trainer\n",
        "    eval_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_eval_batch_size=64,\n",
        "        do_eval=True,\n",
        "        report_to=[\"none\"],\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=loaded_model,\n",
        "        args=eval_args,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation results on loaded LoRA-trained model:\", eval_results)\n",
        "    return eval_results\n",
        "\n",
        "loaded_eval_results = load_and_evaluate(\"./best_checkpoint\", eval_dataset)"
      ],
      "metadata": {
        "id": "_p5zUfGlHnT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: add a few lines of code to check if these evaluation results when loading the checkpoint from disk\n",
        "# is equal to the evaluation results right after training\n",
        "\n",
        "# YOUR CODE HERE\n",
        "for key in eval_results:\n",
        "    orig = eval_results[key]\n",
        "    loaded = loaded_eval_results.get(key)\n",
        "\n",
        "    # float metrics: compare with a tolerance\n",
        "    if isinstance(orig, float):\n",
        "        if not np.isclose(orig, loaded, rtol=1e-6, atol=1e-8):\n",
        "            raise ValueError(f\"Mismatch in metric '{key}': {orig} vs {loaded}\")\n",
        "    else:\n",
        "        # non‐float (e.g. ints or strings): must be exactly equal\n",
        "        if orig != loaded:\n",
        "            raise ValueError(f\"Mismatch in metric '{key}': {orig} vs {loaded}\")\n",
        "\n",
        "raise NotImplementedError()"
      ],
      "metadata": {
        "id": "0zN3UNF8vL3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Analysis and Discussion (12 points)\n",
        "\n",
        "Please answer the following questions in a brief report below.\n",
        "\n",
        "\n",
        "1- What is the optimal rank value for your implementation, and how does it affect the trade-off between parameter count and performance?\n",
        "\n",
        "2- Which layers benefit most from LoRA adaptation? Why do you think this is the case?\n",
        "\n",
        "3- What are the computational advantages of using LoRA compared to full fine-tuning?\n",
        "\n",
        "4- What are potential limitations or drawbacks of LoRA?"
      ],
      "metadata": {
        "id": "KHv8wMLGs4rf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jsWccq5StAu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Your answers to above questions\n",
        "\n",
        "your_answer_1 = \"\"\"1\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "your_answer_2 = \"\"\"1\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "your_answer_3 = \"\"\"Fewer trainable parameters. Lower GPU memory footprint and Faster iteration.\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "your_answer_4 = \"\"\"A low-rank adapter may not capture full-rank weight updates needed for highly specialized tasks, which might under-fit if the true weight update is full-rank.\n",
        "Also it is sensitive for choosing rank and scaling α requires tuning per task and model size.\"\"\"  # <<< YOUR ANSWER HERE >>>\n",
        "\n",
        "if not your_answer_1:\n",
        "  raise NotImplementedError()\n",
        "if not your_answer_2:\n",
        "  raise NotImplementedError()\n",
        "if not your_answer_3:\n",
        "  raise NotImplementedError()\n",
        "if not your_answer_4:\n",
        "  raise NotImplementedError()"
      ],
      "metadata": {
        "id": "gWpo_BOrtBCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nGFFgq4WvsG2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}